{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard library imports\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Third-party imports\n",
    "from lxml import html\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENTH = r'\\([^)]*\\)'\n",
    "QUOTES = r'\\\"[^)]*\\\"'\n",
    "\n",
    "\n",
    "def create_http_session(retry_strategy: Retry = None) -> requests.Session:\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session = requests.Session()\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount (\"http://\", adapter)\n",
    "    return session\n",
    "\n",
    "\n",
    "def _get_track_info(session: requests.Session,\n",
    "                    date: str = None,\n",
    "                    lower_bound: int = 0,\n",
    "                    upper_bound: int = 50) -> tuple:\n",
    "    \n",
    "    # Interval must be in [0, 100]\n",
    "    if (lower_bound < 0 or\n",
    "        upper_bound > 100 or\n",
    "        lower_bound >= upper_bound):\n",
    "        print('Interval must be in [0, 100]...')\n",
    "        raise InvalidInputException\n",
    "\n",
    "    chart_url = 'https://www.billboard.com/charts/hot-100/'\n",
    "    page = session.get(chart_url + date)\n",
    "\n",
    "    '''\n",
    "    Uncomment code below to attempt to get Retry-After header (gives site's rate limiting policy).\n",
    "    Didn't work when I tried it, however.\n",
    "    '''\n",
    "#     if page.status_code == 429:\n",
    "#         print(page)\n",
    "#         time.sleep(int(response.headers[\"Retry-After\"]))\n",
    "    \n",
    "    # Make tree from page DOM\n",
    "    tree = html.fromstring(page.content)\n",
    "    titles_text = '//span[@class=\"chart-element__information__song text--truncate color--primary\"]/text()'\n",
    "    artists_text = '//span[@class=\"chart-element__information__artist text--truncate color--secondary\"]/text()'\n",
    "    \n",
    "    # Scrape top k of Hot 100 via xpaths\n",
    "    titles = tree.xpath(titles_text)[lower_bound:upper_bound]\n",
    "    artists = tree.xpath(artists_text)[lower_bound:upper_bound]\n",
    "    return titles, artists\n",
    "\n",
    "\n",
    "def _clean_title_name(name: str) -> str:\n",
    "    name = name.replace(\"'\", \"\") \\\n",
    "               .split('/', maxsplit=1)[0]\n",
    "    \n",
    "    # Remove parentheticals and quoted names\n",
    "    name = re.sub(PARENTH, '', name)\n",
    "    name = re.sub(QUOTES, '', name)\n",
    "    \n",
    "    # ~99% success rate currently; add more steps here if you want\n",
    "    \n",
    "    return name\n",
    "\n",
    "\n",
    "def _clean_artist_name(name: str) -> str:\n",
    "    name = name.replace(' Featuring', '') \\\n",
    "               .replace(' X ', ' ') \\\n",
    "               .replace(' x', '') \\\n",
    "               .replace(' +', '') \\\n",
    "               .replace(' &', '') \\\n",
    "               .replace(\"'\", '') \\\n",
    "               .replace(\".\", ' ') \\\n",
    "               .split('/', maxsplit=1)[0] \\\n",
    "               .split(' With ', maxsplit=1)[0] \\\n",
    "               .split(' Introducing ', maxsplit=1)[0]\n",
    "        \n",
    "    # Remove parentheticals and quoted names\n",
    "    name = re.sub(PARENTH, '', name)\n",
    "    name = re.sub(QUOTES, '', name)\n",
    "    \n",
    "    # ~99% success rate currently; add more steps here if you want\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def _get_track_uris(titles: list,\n",
    "                    artists: list,\n",
    "                    historical_data: dict,\n",
    "                    spotify_client: spotipy.Spotify,\n",
    "                    prev_week: str = None,\n",
    "                    debug: bool = False,\n",
    "                    record_misses_list: list = None,\n",
    "                    day_of_the_week: int = 2) -> tuple:\n",
    "\n",
    "    # Historical data must be a valid dataset\n",
    "    if len(historical_data) > 0 and not valid_dataset(historical_data):\n",
    "        print('Historical data must be a valid dataset')\n",
    "        raise InvalidInputException\n",
    "\n",
    "    # Build queries\n",
    "    queries = ['track:' + title + ' artist:' + artist\n",
    "               for title, artist in zip(titles, artists)]\n",
    "    \n",
    "    # Get previous week's queries and uris\n",
    "    if prev_week:\n",
    "        prev_queries = {item['query']: item['uri'] for item in historical_data[prev_week]}\n",
    "    \n",
    "    # Search for uris via Spotify Web API\n",
    "    uris = []\n",
    "    for i, query in enumerate(queries):\n",
    "        # Use previous week's uri\n",
    "        if prev_week and query in prev_queries:\n",
    "            uri = prev_queries[query]\n",
    "        else:\n",
    "            found_tracks = spotify_client.search(q=query, type='track')\n",
    "            items = found_tracks['tracks']['items']\n",
    "            # Take first (most popular) uri\n",
    "            if len(items) > 0:\n",
    "                uri = items[0]['uri']\n",
    "            else:\n",
    "                # Try the track query plus only the first part of artist query\n",
    "                new_query = 'track:' + titles[i] + ' artist:'\n",
    "                if len(artists[i]) > 0:\n",
    "                    trunc_artist = artists[i].split()[0]\n",
    "                    new_query += trunc_artist\n",
    "                found_tracks = spotify.search(q=new_query, type='track')\n",
    "                items = found_tracks['tracks']['items']\n",
    "                if len(items) > 0:\n",
    "                    uri = items[0]['uri']\n",
    "                # New query is still unsuccessful...set uri to None\n",
    "                else:\n",
    "                    if record_misses_list is not None:\n",
    "                        prev_misses = [item[-1] for item in record_misses_list]\n",
    "                        if query not in prev_misses:\n",
    "                            current_week = datetime.date.today()\n",
    "                            current_day = current_week.weekday()\n",
    "                            if current_day != day_of_the_week:\n",
    "                                current_week += datetime.timedelta(days=day_of_the_week - current_day)\n",
    "                            record_misses_list.append((i, current_week, query))\n",
    "                    if debug:\n",
    "                        print(i, new_query)\n",
    "                    uri = None\n",
    "\n",
    "        uris.append(uri)\n",
    "\n",
    "    return queries, uris\n",
    "\n",
    "\n",
    "def build_dataset(start_date: datetime.date,\n",
    "                  end_date: datetime.date,\n",
    "                  spotify_client: spotipy.Spotify,\n",
    "                  http_session: requests.Session,\n",
    "                  historical_data: dict = None,\n",
    "                  refresh: bool = True,\n",
    "                  debug: bool = False,\n",
    "                  record_misses_list: list = None,\n",
    "                  top_k: int = 50,\n",
    "                  day_of_the_week: int = None) -> dict:\n",
    "    \n",
    "    dataset = historical_data if historical_data is not None else {}\n",
    "    # Historical data must be a valid dataset\n",
    "    if len(dataset) > 0 and not valid_dataset(dataset):\n",
    "        print('Historical data must be a valid dataset')\n",
    "        raise InvalidInputException\n",
    "\n",
    "    # Convert dates to datetime.date objects\n",
    "    try:\n",
    "        if not isinstance(start_date, datetime.date):\n",
    "            start_date = datetime.date.fromisoformat(start_date)\n",
    "        if not isinstance(end_date, datetime.date):\n",
    "            end_date = datetime.date.fromisoformat(end_date)\n",
    "    except ValueError:\n",
    "        print('Invalid start or end dates')\n",
    "        raise InvalidInputException\n",
    "\n",
    "    # Either extend or refresh the historical data\n",
    "    if len(dataset) > 0:\n",
    "        index = 0 if refresh else -1\n",
    "        first_week = datetime.date.fromisoformat(list(dataset)[index])\n",
    "        if refresh and first_week < start_date:\n",
    "            start_date = first_week\n",
    "    \n",
    "    # Verify that dates are valid and restrict them to reasonable values\n",
    "    if (start_date > end_date):\n",
    "        print(\"Start date must be on or before end date...\")\n",
    "        raise InvalidInputException\n",
    "    if (start_date < datetime.date(1960, 1, 1)):\n",
    "        start_date = datetime.date(1960, 1, 1)\n",
    "    if (end_date > datetime.date.today()):\n",
    "        end_date = datetime.date.today()\n",
    "    \n",
    "    # Center weeks on provided day of the week\n",
    "    if day_of_the_week:\n",
    "        if end_date.weekday() != day_of_the_week:\n",
    "            end_date += datetime.timedelta(days=day_of_the_week - end_date.weekday())\n",
    "        if start_date.weekday() != day_of_the_week:\n",
    "            start_date += datetime.timedelta(days=day_of_the_week - start_date.weekday())\n",
    "    \n",
    "    # Find all dates b/w start and end\n",
    "    week = datetime.timedelta(weeks=1)\n",
    "    dates = [str(start_date)]\n",
    "    while start_date <= end_date:\n",
    "        start_date += week\n",
    "        if start_date > end_date:\n",
    "            break\n",
    "        dates.append(str(start_date))\n",
    "\n",
    "    # Build the date dict and return it\n",
    "    for i, date in enumerate(dates, 1):\n",
    "        # If refreshing, re-use titles and artists to prevent http GET request\n",
    "        if refresh and date in dataset and top_k <= len(dataset[date]):\n",
    "            titles, artists = zip(*[(item['title'], item['artist']) for item in dataset[date]])\n",
    "        else:\n",
    "            titles, artists = _get_track_info(session=http_session,\n",
    "                                              date=date,\n",
    "                                              upper_bound=top_k)\n",
    "        \n",
    "        # Reformat title and artist strings to be compatible with Spotify API search\n",
    "        clean_titles = [_clean_title_name(name) for name in titles]\n",
    "        clean_artists = [_clean_artist_name(name) for name in artists]\n",
    "        prev_week = str(datetime.date.fromisoformat(date) - week) if i > 1 else None\n",
    "        queries, uris = _get_track_uris(clean_titles,\n",
    "                                        clean_artists,\n",
    "                                        dataset,\n",
    "                                        spotify_client,\n",
    "                                        prev_week=prev_week,\n",
    "                                        debug=debug,\n",
    "                                        record_misses_list=record_misses_list,\n",
    "                                        day_of_the_week=day_of_the_week)\n",
    "        dataset[date] = [{'title': title,\n",
    "                          'artist': artist,\n",
    "                          'query': query,\n",
    "                          'uri': uri} \n",
    "                          for title, artist, query, uri in zip(titles, artists, queries, uris)]\n",
    "        if debug:\n",
    "            print(f'Finished Date {i} ({date}) of {len(dates)}')\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_mongo_dataset(dataset: dict) -> dict:\n",
    "    \n",
    "    # Dataset must be valid\n",
    "    if not valid_dataset(dataset):\n",
    "        raise InvalidInputException\n",
    "\n",
    "    # Less direct access to uris, but compatible with mongodb\n",
    "    return [{'_id': date,\n",
    "             'ranking': dataset[date]}\n",
    "             for date\n",
    "             in dataset]\n",
    "\n",
    "\n",
    "def get_mongo_spotify_info(spotify_info_dict: dict) -> dict:\n",
    "    \n",
    "    # Spotify info must be valid\n",
    "    if not valid_spotify_info(spotify_info_dict):\n",
    "        raise InvalidInputException\n",
    "    \n",
    "    # Less direct access to info, but compatible with mongodb\n",
    "    return [dict(_id=uri, **spotify_info_dict[uri])\n",
    "            for uri in spotify_info_dict]\n",
    "\n",
    "\n",
    "def save_dataset_as_json(dataset: dict,\n",
    "                         path: str = os.getcwd(),\n",
    "                         indent: int = None,\n",
    "                         mongodb: bool = False) -> None:\n",
    "    \n",
    "    # Dataset must be valid\n",
    "    if not valid_dataset(dataset):\n",
    "        raise InvalidInputException\n",
    "\n",
    "    # Verify existence of path\n",
    "    if not os.path.exists(path):\n",
    "        print(f'Given path \"{path}\" not found, could be a path formatting or permission error')\n",
    "        return\n",
    "    \n",
    "    # Format file string\n",
    "    file_string = 'billboard_uris_ranking'\n",
    "    if mongodb: # mongodb needs to be compact\n",
    "        indent = None\n",
    "        file_string += '_mongodb'\n",
    "    if indent:\n",
    "        file_string += '_indented'\n",
    "    \n",
    "    \n",
    "    # Record range and day of the week of data\n",
    "    dates = list(dataset)\n",
    "    start_date = dates[0]\n",
    "    end_date = dates[-1]\n",
    "    file_string += f'_{start_date}_to_{end_date}_{datetime.date.fromisoformat(start_date).strftime(\"%a\").lower()}'\n",
    "\n",
    "    # Version the file to prevent overwriting\n",
    "    file_version = len(glob.glob(file_string + '*.json'))\n",
    "    file_string += f'_v{file_version}.json'\n",
    "    \n",
    "    # Save dict as file_string\n",
    "    with open(os.path.join(path, file_string), 'w', encoding='utf-8') as f:\n",
    "        # Format dict if necessary\n",
    "        save_dict = get_mongo_dataset(dataset) if mongodb else dataset\n",
    "        json.dump(save_dict, f, ensure_ascii=False, indent=indent)\n",
    "\n",
    "\n",
    "def save_spotify_info_as_json(spotify_info_dict: dict,\n",
    "                              path: str = os.getcwd(),\n",
    "                              indent: int = None,\n",
    "                              mongodb: bool = False) -> None:\n",
    "\n",
    "    # Spotify info must be valid\n",
    "    if not valid_spotify_info(spotify_info_dict):\n",
    "        raise InvalidInputException\n",
    "\n",
    "    # Verify existence of path\n",
    "    if not os.path.exists(path):\n",
    "        print(f'Given path \"{path}\" not found, could be a path formatting or permission error')\n",
    "        return\n",
    "    \n",
    "    # Format file string\n",
    "    file_string = 'billboard_uris_spotify_info'\n",
    "    if mongodb: # mongodb needs to be compact\n",
    "        indent = None\n",
    "        file_string += '_mongodb'\n",
    "    if indent:\n",
    "        file_string += '_indented'\n",
    "    \n",
    "    # Version the file to prevent overwriting\n",
    "    file_version = len(glob.glob(file_string + '*.json'))\n",
    "    file_string += f'_v{file_version}.json'\n",
    "    \n",
    "    # Save dict as file_string\n",
    "    with open(os.path.join(path, file_string), 'w', encoding='utf-8') as f:\n",
    "        # Format dict if necessary\n",
    "        save_dict = get_mongo_spotify_info(spotify_info_dict) if mongodb else spotify_info_dict\n",
    "        json.dump(save_dict, f, ensure_ascii=False, indent=indent)\n",
    "        \n",
    "\n",
    "def manual_add_uri(dataset: dict,\n",
    "                   uri: str = None,\n",
    "                   query: str = None) -> None:\n",
    "    \n",
    "    # Dataset must be valid\n",
    "    if not valid_dataset(dataset):\n",
    "        raise InvalidInputException\n",
    "    \n",
    "    # Add the uri corresponding to query\n",
    "    for date in dataset:\n",
    "        for item in dataset[date]:\n",
    "            if uri and query and item['query'] == query:\n",
    "                item['uri'] = uri\n",
    "\n",
    "                \n",
    "def manual_add_uris(dataset: dict,\n",
    "                    uri_dict: dict = None) -> None:\n",
    "    \n",
    "    # Dataset must be valid and uris must be in dict\n",
    "    if not (valid_dataset(dataset) and isinstance(uri_dict, dict)):\n",
    "        raise InvalidInputException\n",
    "    \n",
    "    # Add each uri in the dict to the dataset\n",
    "    for query in uri_dict:\n",
    "        manual_add_uri(dataset=dataset,\n",
    "                       uri=uri_dict[query],\n",
    "                       query=query)\n",
    "        \n",
    "\n",
    "def get_unique_uris(dataset: dict) -> set:\n",
    "\n",
    "    # Dataset must be valid\n",
    "    if not valid_dataset(dataset):\n",
    "        raise InvalidInputException\n",
    "\n",
    "    return {item['uri'] for date in dataset for item in dataset[date] if item['uri']}\n",
    "\n",
    "\n",
    "def get_spotify_info(uris: set,\n",
    "                     artist_info: bool = False,\n",
    "                     audio_features: bool = False) -> dict:\n",
    "    \n",
    "    # Verify that uris are valid\n",
    "    if not valid_uris(uris):\n",
    "        raise InvalidInputException\n",
    "\n",
    "   # Build dict\n",
    "    if audio_features:\n",
    "        spotify_info_dict = {uri: {'track_info': spotify.track(uri),\n",
    "                                   'audio_features': spotify.audio_features(uri)[0]}\n",
    "                                   for uri in uris}\n",
    "    else:\n",
    "        spotify_info_dict = {uri: {'track_info': spotify.track(uri)}\n",
    "                             for uri in uris}  \n",
    "    if artist_info:\n",
    "        for uri in spotify_info_dict:\n",
    "            artist_uri = spotify_info_dict[uri]['track_info']['artists'][0]['uri']\n",
    "            spotify_info_dict[uri]['artist_info'] = spotify.artist(artist_uri)\n",
    "\n",
    "    return spotify_info_dict\n",
    "    \n",
    "\n",
    "def add_spotify_info_inline(dataset: dict,\n",
    "                            uris: set = None,\n",
    "                            spotify_info_dict: dict = None,\n",
    "                            audio_features: bool = False) -> None:\n",
    "\n",
    "    # Inputs must be valid\n",
    "    if not (valid_dataset(dataset) and\n",
    "            valid_spotify_info(spotify_info_dict) and\n",
    "            valid_uris(uris)):\n",
    "        raise InvalidInputException\n",
    "    \n",
    "    # Get necessary parts if not provided and augment dataset\n",
    "    if not uris:\n",
    "        uris = get_unique_uris(dataset)\n",
    "    if not spotify_info_dict:\n",
    "        spotify_info_dict = get_spotify_info(uris, audio_features=audio_features)\n",
    "    for date in dataset:\n",
    "        for item in dataset[date]:\n",
    "            item['spotify_info'] = spotify_info_dict[item['uri']] if item['uri'] else None\n",
    "            \n",
    "\n",
    "def valid_uris(uris: set, uri_type: str = 'track') -> bool:\n",
    "\n",
    "    # Uris must be iterable\n",
    "    try:\n",
    "        iterator = iter(uris)\n",
    "    except TypeError:\n",
    "        return False\n",
    "    \n",
    "    # Can't contain None type\n",
    "    if None in uris:\n",
    "        print(f'An element of \"{uris}\" has type \"None\", remove it and try again')\n",
    "        return False\n",
    "    \n",
    "    # Spotify uris must be valid\n",
    "    for uri in uris:\n",
    "        uri_parts = uri.split(':')\n",
    "        if uri_parts[0] != 'spotify' or uri_parts[1] != uri_type:\n",
    "            print(f'Invalid uri {uri}')\n",
    "            return False\n",
    "    \n",
    "    # Passed all checks; valid uri container\n",
    "    return True\n",
    "\n",
    "\n",
    "def valid_dataset(dataset: dict) -> bool:\n",
    "    \n",
    "    # Dataset must be a dict\n",
    "    if not isinstance(dataset, dict):\n",
    "        return False\n",
    "    \n",
    "    for date in dataset:\n",
    "        # Keys must be ISO formatted dates (YYYY-MM-DD)\n",
    "        try:\n",
    "            datetime.date.fromisoformat(date)\n",
    "        except ValueError:\n",
    "            print(f'Invalid date string {date}')\n",
    "            return False\n",
    "        # Values must be iterable\n",
    "        try:\n",
    "            iterator = iter(dataset[date])\n",
    "        except TypeError:\n",
    "            print(f'Value {dataset[date]} is not iterable')\n",
    "            return False\n",
    "        # Each item must be a dict with the required keys\n",
    "        required_keys = {'title', 'artist', 'query', 'uri'}\n",
    "        for item in dataset[date]:\n",
    "            if not (isinstance(item, dict) and\n",
    "                    all(key in item for key in required_keys)\n",
    "            ):\n",
    "                print(f'Item {item} is not a dict or does not have all required keys')\n",
    "                return False\n",
    "\n",
    "    # Passed all checks; valid dataset\n",
    "    return True\n",
    "\n",
    "\n",
    "def valid_spotify_info(spotify_info_dict: dict) -> bool:\n",
    "\n",
    "    # Dataset must be a dict\n",
    "    if not isinstance(spotify_info_dict, dict):\n",
    "        print(f'Item {spotify_info_dict} is not a dict')\n",
    "        return False\n",
    "    \n",
    "    # Keys must be valid uris and values must be dicts\n",
    "    for uri in spotify_info_dict:\n",
    "        uri_parts = uri.split(':')\n",
    "        if uri_parts[0] != 'spotify' or uri_parts[1] != 'track':\n",
    "            print(f'Invalid uri {uri}')\n",
    "            return False\n",
    "        if not isinstance(spotify_info_dict[uri], dict):\n",
    "            print(f'Item {spotify_info_dict[uri]} is not a dict')\n",
    "            return False\n",
    "        \n",
    "    # Passed all checks; valid spotify info\n",
    "    return True\n",
    "\n",
    "def update_mongo():\n",
    "\n",
    "    # Get latest week's Top 50\n",
    "    misses = []\n",
    "    new_data = build_dataset(historical_data=None,\n",
    "                             start_date=datetime.date.today(),\n",
    "                             end_date=datetime.date.today(),\n",
    "                             spotify_client=spotify,\n",
    "                             http_session=http,\n",
    "                             refresh=False,\n",
    "                             debug=True,\n",
    "                             record_misses_list=misses,\n",
    "                             top_k=50,\n",
    "                             day_of_the_week=2)\n",
    "\n",
    "    most_recent_date = list(new_data)[-1]\n",
    "\n",
    "    # Prep result for mongodb upload\n",
    "    mongo_new_data = {'_id': most_recent_date,\n",
    "                      'ranking': new_data[most_recent_date]}\n",
    "\n",
    "    new_uris = get_unique_uris(new_data)\n",
    "\n",
    "    # Determine which uris are new\n",
    "    with open(os.path.join(os.getcwd(), '..', 'json-files', 'unique_uris.pickle'), 'rb') as f:\n",
    "        uris = pickle.load(f)\n",
    "    input_uris = new_uris - uris\n",
    "    uris = uris | input_uris\n",
    "\n",
    "    # Get info for those new uris and prep for mongodb upload\n",
    "    new_spotify_info = get_spotify_info(input_uris,\n",
    "                                        artist_info=True,\n",
    "                                        audio_features=True)\n",
    "\n",
    "    mongo_new_spotify_info = get_mongo_spotify_info(new_spotify_info)\n",
    "\n",
    "    # Upload results to mongodb\n",
    "    client = MongoClient(os.environ['ATLAS_CONNECT'])\n",
    "    db = client.data\n",
    "    spotify_info = db.spotify_info\n",
    "    billboard_rankings = db.billboard_rankings\n",
    "    spotify_info.insert_many(get_mongo_spotify_info(new_spotify_info))\n",
    "    billboard_rankings.insert_one(mongo_new_data)\n",
    "\n",
    "    # Update and save query misses\n",
    "    with open(os.path.join(os.getcwd(), '..', 'json-files', 'query_misses.pickle'), 'rb') as f:\n",
    "        query_misses = pickle.load(f)\n",
    "    query_misses += misses[0]\n",
    "    with open(os.path.join(os.getcwd(), '..', 'json-files', 'query_misses.pickle'), 'wb') as f:\n",
    "        pickle.dump(query_misses, f)\n",
    "        \n",
    "    with open(os.path.join(os.getcwd(), '..', 'json-files', 'unique_uris.pickle'), 'wb') as f:\n",
    "        pickle.dump(uris, f)\n",
    "        \n",
    "\n",
    "class InvalidInputException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials())\n",
    "\n",
    "retry_strategy = Retry(\n",
    "    total=5,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"],\n",
    "    backoff_factor=2\n",
    ")\n",
    "http = create_http_session(retry_strategy=retry_strategy)\n",
    "\n",
    "# client_id = os.environ['SPOTIPY_CLIENT_ID']\n",
    "# client_secret = os.environ['SPOTIPY_CLIENT_SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in manual_dict:\n",
    "    manual_add_uri(data,\n",
    "                   uri=manual_dict[query],\n",
    "                   query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify.recommendation_genre_seeds();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify.recommendations(seed_tracks=list(uris)[:5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(os.environ['ATLAS_CONN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_info = db.spotify_info\n",
    "billboard_rankings = db.billboard_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x1775013a8c0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billboard_rankings.update_one({'_id': '2020-12-02'}, {'$set': {'ranking.24.uri': 'spotify:track:0SErdEdRcVX1uJCf1eTGYH'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spotify_info = get_spotify_info({'spotify:track:0SErdEdRcVX1uJCf1eTGYH'},\n",
    "                                    artist_info=True,\n",
    "                                    audio_features=True)\n",
    "\n",
    "mongo_new_spotify_info = get_mongo_spotify_info(new_spotify_info)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x1774f001d80>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_info.insert_one(mongo_new_spotify_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 track:Somebodys Problem artist:Morgan\n",
      "Finished Date 1 (2020-12-02) of 1\n"
     ]
    }
   ],
   "source": [
    "# Get latest week's Top 50\n",
    "misses = []\n",
    "new_data = build_dataset(historical_data=None,\n",
    "                         start_date=datetime.date.today(),\n",
    "                         end_date=datetime.date.today(),\n",
    "                         spotify_client=spotify,\n",
    "                         http_session=http,\n",
    "                         refresh=False,\n",
    "                         debug=True,\n",
    "                         record_misses_list=misses,\n",
    "                         top_k=50,\n",
    "                         day_of_the_week=2)\n",
    "\n",
    "most_recent_date = list(new_data)[-1]\n",
    "\n",
    "# Prep result for mongodb upload\n",
    "mongo_new_data = {'_id': most_recent_date,\n",
    "                  'ranking': new_data[most_recent_date]}\n",
    "\n",
    "new_uris = get_unique_uris(new_data)\n",
    "\n",
    "# Update and save query misses\n",
    "with open(os.path.join(os.getcwd(), '..', 'json-files', 'query_misses.pickle'), 'rb') as f:\n",
    "    query_misses = pickle.load(f)\n",
    "query_misses += misses[0]\n",
    "with open(os.path.join(os.getcwd(), '..', 'json-files', 'query_misses.pickle'), 'wb') as f:\n",
    "    pickle.dump(query_misses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), '..', 'json-files', 'query_misses.pickle'), 'rb') as f:\n",
    "    query_misses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info for those new uris and prep for mongodb upload\n",
    "new_spotify_info = get_spotify_info(new_uris,\n",
    "                                    artist_info=True,\n",
    "                                    audio_features=True)\n",
    "\n",
    "mongo_new_spotify_info = get_mongo_spotify_info(new_spotify_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in mongo_new_spotify_info:\n",
    "    try:\n",
    "        spotify_info.insert_one(item)\n",
    "    except DuplicateKeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x17750752e00>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billboard_rankings.insert_one(mongo_new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
